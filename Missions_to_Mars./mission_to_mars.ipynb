{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import selenium\n",
    "import splinter\n",
    "from bs4 import BeautifulSoup as bs \n",
    "from splinter import Browser\n",
    "import requests\n",
    "import time\n",
    "import tweepy\n",
    "import json\n",
    "import re\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "import twitter_credentials\n",
    "\n",
    "def init_browser():\n",
    "    #starting the browser\n",
    "    executable_path = {'executable_path':'/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "def close_browser(browser):\n",
    "    browser.quit()\n",
    "\n",
    "def scrape_all():\n",
    "    mars = {}\n",
    "    mars['news_info'] = scrape_news()\n",
    "    mars['feat_img'] = f_img()\n",
    "    mars['twitter'] = twitter()\n",
    "    mars['data_table'] = table()\n",
    "    mars['hemisphere_image_urls'] = mars_h_images()\n",
    "    return  mars\n",
    "def scrape_news():\n",
    "    try:\n",
    "        ## initialize browser and latest_news dict\n",
    "        browser = init_browser()\n",
    "        latest_news={}\n",
    "\n",
    "\n",
    "        ## define url and command browser to visit and parse HTML for posts tag   \n",
    "        nasa_url= \"https://mars.nasa.gov\"\n",
    "        news_url= \"https://mars.nasa.gov/news/\"\n",
    "        browser.visit(news_url)\n",
    "        time.sleep(1)\n",
    "    \n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        try:\n",
    "            #find first news article \n",
    "            news = soup.find('div', class_='list_text')\n",
    "\n",
    "        \n",
    "            ## collect title, date, description and link of post\n",
    "            titles = news.find('div', class_='content_title').text\n",
    "            date = news.find('div', class_='list_date').text\n",
    "            news_p = news.find('div', class_='article_teaser_body').text\n",
    "            link = news.a['href']\n",
    "            ## run it untill the attribute error doesnt show up. \n",
    "        except AttributeError:\n",
    "            print(\"error\")\n",
    "            \n",
    "            \n",
    "\n",
    "        ## add results to latest_news dictionary\n",
    "        latest_news['news_title'] = titles\n",
    "        latest_news['news_date'] = date\n",
    "        latest_news['news_about'] = news_p\n",
    "        latest_news[\"news_link\"] = f'{nasa_url}{link}'\n",
    "        return latest_news\n",
    "    \n",
    "## close browser    \n",
    "    finally:        \n",
    "        close_browser(browser)\n",
    "def f_img():\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "\n",
    "        url2 = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "        browser.visit(url2)\n",
    "\n",
    "        #get html code from it \n",
    "        html2 = browser.html\n",
    "        soup2 = bs(html2,'html.parser')\n",
    "\n",
    "        #get the ending of the url to link the image\n",
    "        result2 = soup2.find('article', class_ = \"carousel_item\")\n",
    "        #identify and reutnr link to listing \n",
    "        link = result2.a['data-fancybox-href']\n",
    "        featured_image_url = f'https://www.jpl.nasa.gov{link}'\n",
    "        return featured_image_url\n",
    "\n",
    "    finally:        \n",
    "        close_browser(browser)\n",
    "\n",
    "def twitter():\n",
    "    # set up dict\n",
    "    mars_weather = {}\n",
    "    # set up the authication fro the twitter API\n",
    "    auth = tweepy.OAuthHandler(twitter_credentials.Cons_API_key, twitter_credentials.Cons_API_secret_key )\n",
    "    auth.set_access_token(twitter_credentials.Access_token, twitter_credentials.Access_token_secret)\n",
    "    #chose the API of twitter which we will use JSO \n",
    "    api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())\n",
    "\n",
    "    target_user = \"MarsWxReport\"\n",
    "    tweet = api.user_timeline(target_user, count =0, page = 0)\n",
    "    try:\n",
    "    \n",
    "        link = ((tweet)[0]['entities']['urls'][0]['url'])\n",
    "        mars_weather['link'] = link\n",
    "    except IndexError:\n",
    "        print('no url')\n",
    "        \n",
    "    text = ((tweet)[0]['text'])\n",
    "    mars_weather['tweet'] = text.replace(link,\"\")\n",
    "    \n",
    "    return mars_weather\n",
    "def table():\n",
    "     #read html to get mars facts\n",
    "    mars_df = pd.read_html(\"https://space-facts.com/mars/\")[0]\n",
    "    mars_df.columns = ['Description', 'Value']\n",
    "    mars_df.set_index(\"Description\", inplace = True)\n",
    "    mars_facts = mars_df.to_html(index = True, header = True, classes=\"table table-striped\")\n",
    "    return mars_facts\n",
    "def mars_h_images():\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        #go to the url to get Mars Hemisphere images \n",
    "        url5 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "        browser.visit(url5)\n",
    "\n",
    "        #get html code from it \n",
    "        html2 = browser.html\n",
    "        soup5 = bs(html2, \"html.parser\")\n",
    "        #find all the listings in the html\n",
    "        result5 = soup5.find_all('div', class_='item')\n",
    "         #create an empty list \n",
    "        hemisphere_image_urls =[]\n",
    "        #iterate through the four results and go to their link to get the image url\n",
    "        for result in result5:\n",
    "            link = result.a['href']\n",
    "            browser.visit('https://astrogeology.usgs.gov'+link)\n",
    "            html6 = browser.html\n",
    "            soup6 = bs(html6,'html.parser')\n",
    "            first = soup6.find('div', class_ = 'downloads')\n",
    "            image_url =first.a['href']\n",
    "            title = soup6.find('h2', class_ ='title').text\n",
    "            post = {'title': title, 'img_url': image_url}\n",
    "            hemisphere_image_urls.append(post)\n",
    "            browser.back\n",
    "        return hemisphere_image_urls\n",
    "    finally:        \n",
    "        close_browser(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'news_info'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c797f5f24434>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnews_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'news_info'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mars_df = pd.read_html(\"https://space-facts.com/mars/\")[0]\n",
    "mars_df.columns = ['Description', 'Value']\n",
    "mars_df.set_index(\"Description\", inplace = True)\n",
    "mars_facts = mars_df.to_html(index = True, header = True, classes=\"table table-striped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table1 = pd.read_html(\"https://space-facts.com/mars/\")[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2 = pd.DataFrame(table1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.columns = ['Description', 'Value']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table2.set_index(\"Description\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_facts = table2.to_html(index = True, header = True, classes=\"table table-striped\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Description</th>\n",
       "      <th>Value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Equatorial Diameter:</td>\n",
       "      <td>6,792 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Polar Diameter:</td>\n",
       "      <td>6,752 km</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Mass:</td>\n",
       "      <td>6.39 × 10^23 kg (0.11 Earths)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Moons:</td>\n",
       "      <td>2 (Phobos &amp; Deimos)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Orbit Distance:</td>\n",
       "      <td>227,943,824 km (1.38 AU)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>Orbit Period:</td>\n",
       "      <td>687 days (1.9 years)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Surface Temperature:</td>\n",
       "      <td>-87 to -5 °C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>First Record:</td>\n",
       "      <td>2nd millennium BC</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>Recorded By:</td>\n",
       "      <td>Egyptian astronomers</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Description                          Value\n",
       "0  Equatorial Diameter:                       6,792 km\n",
       "1       Polar Diameter:                       6,752 km\n",
       "2                 Mass:  6.39 × 10^23 kg (0.11 Earths)\n",
       "3                Moons:            2 (Phobos & Deimos)\n",
       "4       Orbit Distance:       227,943,824 km (1.38 AU)\n",
       "5         Orbit Period:           687 days (1.9 years)\n",
       "6  Surface Temperature:                   -87 to -5 °C\n",
       "7         First Record:              2nd millennium BC\n",
       "8          Recorded By:           Egyptian astronomers"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go to the url nasa mars page to web scrape\n",
    "url = 'https://mars.nasa.gov/news/?page=0&per_page=40&order=publish_date+desc%2Ccreated_at+desc&search=&category=19%2C165%2C184%2C204&blank_scope=Latest'\n",
    "browser.visit(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code from the webpage and parse it \n",
    "html = browser.html\n",
    "soup = BeautifulSoup(html,'html.parser')\n",
    "\n",
    "#get the first news title and first news summary \n",
    "result = soup.find('div',  class_ = 'list_text')\n",
    "\n",
    "news_title = result.find('a', target = '_self').text\n",
    "news_p = result.find('div', class_='article_teaser_body').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#go to the url for the featured image \n",
    "url2 = 'https://www.jpl.nasa.gov/spaceimages/?search=&category=Mars'\n",
    "browser.visit(url2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code from it \n",
    "html2 = browser.html\n",
    "soup2 = BeautifulSoup(html2,'html.parser')\n",
    "\n",
    "#get the ending of the url to link to the image \n",
    "result2 = soup2.find('article',  class_ = 'carousel_item')\n",
    "class1 = result2.find('a', class_=\"button fancybox\").text\n",
    "# Identify and return link to listing\n",
    "link = result.a['data-fancybox-href']\n",
    "\n",
    "featured_image_url = 'https://www.jpl.nasa.gov+link'+ link\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "request.get('https://twitter.com/marswxreport?lang=en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the twitter of mars weather \n",
    "url3 = 'https://twitter.com/marswxreport?lang=en'\n",
    "browser.visit(url3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html = requests.get('https://twitter.com/marswxreport?lang=en').text\n",
    "html1 = BeautifulSoup(html,'html.parser')\n",
    "news1 = html1.find('p',  class_ = \"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\")\n",
    "next_text = news1.text\n",
    "news5 = news1.find_all(\"a\")\n",
    "text=(\"\")\n",
    "text1=(\"\")\n",
    "for news in news5:\n",
    "    text1 = news.text\n",
    "    text = text+text1\n",
    "mars_weather = next_text.replace(text,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info = scrape_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news1.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news = news.replace(news5,'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code from it \n",
    "html3 = browser.html\n",
    "soup3 = BeautifulSoup(html3,'html.parser')\n",
    "\n",
    "#get the first tweet on the page \n",
    "result3 = soup3.find('div',  class_ = 'js-tweet-text-container')\n",
    "mars_weather = result3.find('p', class_=\"TweetTextSize TweetTextSize--normal js-tweet-text tweet-text\").text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the url to get mars facts\n",
    "url4 = 'https://space-facts.com/mars/'\n",
    "browser.visit(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code from it \n",
    "html4 = browser.html\n",
    "soup4 = BeautifulSoup(html4,'html.parser')\n",
    "\n",
    "#get the first tweet on the page \n",
    "result4 = soup4.find('table')\n",
    "mars_facts = result4.to_html(header = False, index = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars_df = pd.read_html(\"https://space-facts.com/mars/\")[0]\n",
    "mars_df.columns=[\"Description\", \"Value\"]\n",
    "mars_df.set_index(\"Description\", inplace=True)\n",
    "mars_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go to the url to get Mars Hemispheres images\n",
    "url5 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "browser.visit(url5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get html code from it \n",
    "html5 = browser.html\n",
    "soup5 = BeautifulSoup(html5,'html.parser')\n",
    "\n",
    "#find all  the listings in the html \n",
    "result5 = soup5.find_all('div', class_=\"item\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an empty list \n",
    "hemisphere_image_urls = []\n",
    "# iterate through the four results and go to their link to get the image url\n",
    "for result in result5:\n",
    "    link = result.a['href']\n",
    "    browser.visit('https://astrogeology.usgs.gov'+link)\n",
    "    html6 = browser.html\n",
    "    soup6 = BeautifulSoup(html6,'html.parser')\n",
    "    first = soup6.find('div',class_=\"downloads\")\n",
    "    image_url = first.a['href']\n",
    "    title = soup6.find('h2', class_= 'title').text\n",
    "    post = {\"title\": title, \"img_url\": image_url }\n",
    "    hemisphere_image_urls.append(post)\n",
    "    \n",
    "    browser.back()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hemisphere_image_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
