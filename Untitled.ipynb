{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import splinter\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from splinter import Browser\n",
    "import requests\n",
    "import time\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_news():\n",
    "    try:\n",
    "        ## initialize browser and latest_news dict\n",
    "        browser = init_browser()\n",
    "        latest_news={}\n",
    "\n",
    "\n",
    "        ## define url and command browser to visit and parse HTML for posts tag   \n",
    "        nasa_url= \"https://mars.nasa.gov\"\n",
    "        news_url= \"https://mars.nasa.gov/news/\"\n",
    "        browser.visit(news_url)\n",
    "        time.sleep(1)\n",
    "    \n",
    "        html = browser.html\n",
    "        soup = bs(html, \"html.parser\")\n",
    "        try:\n",
    "            #find first news article \n",
    "            news = soup.find('div', class_='list_text')\n",
    "\n",
    "        \n",
    "            ## collect title, date, description and link of post\n",
    "            titles = news.find('div', class_='content_title').text\n",
    "            date = news.find('div', class_='list_date').text\n",
    "            news_p = news.find('div', class_='article_teaser_body').text\n",
    "            link = news.a['href']\n",
    "            \n",
    "        except AttributeError as error:\n",
    "            print(\"error\")\n",
    "            \n",
    "            \n",
    "\n",
    "        ## add results to latest_news dictionary\n",
    "        latest_news['news_title'] = titles\n",
    "        latest_news['news_date'] = date\n",
    "        latest_news['news_about'] = news_p\n",
    "        latest_news[\"news_link\"] = f'{nasa_url}{link}'\n",
    "        return latest_news\n",
    "    \n",
    "## close browser    \n",
    "    finally:        \n",
    "        close_browser(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mars_h_images():\n",
    "    try:\n",
    "        browser = init_browser()\n",
    "        #go to the url to get Mars Hemisphere images \n",
    "        url5 = 'https://astrogeology.usgs.gov/search/results?q=hemisphere+enhanced&k1=target&v1=Mars'\n",
    "        browser.visit(url5)\n",
    "\n",
    "        #get html code from it \n",
    "        html2 = browser.html\n",
    "        soup5 = bs(html2, \"html.parser\")\n",
    "        #find all the listings in the html\n",
    "        result5 = soup5.find_all('div', class_='item')\n",
    "         #create an empty list \n",
    "        hemisphere_image_urls =[]\n",
    "        #iterate through the four results and go to their link to get the image url\n",
    "        for result in result5:\n",
    "            link = result.a['href']\n",
    "            browser.visit('https://astrogeology.usgs.gov'+link)\n",
    "            html6 = browser.html\n",
    "            soup6 = bs(html6,'html.parser')\n",
    "            first = soup6.find('div', class_ = 'downloads')\n",
    "            image_url =first.a['href']\n",
    "            title = soup6.find('h2', class_ ='title').text\n",
    "            post = {'title': title, 'img_url': image_url}\n",
    "            hemisphere_image_urls.append(post)\n",
    "            browser.back\n",
    "        return hemisphere_image_urls\n",
    "    finally:        \n",
    "        close_browser(browser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = scrape_news()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y['news_title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mars ={}\n",
    "mars[\"info\"] = y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    #starting the browser\n",
    "    executable_path = {'executable_path':'/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome', **executable_path, headless=True)\n",
    "\n",
    "def close_browser(browser):\n",
    "    browser.quit()\n",
    "\n",
    "def news_scrape():\n",
    "    browser = init_browser()\n",
    "    url = 'https://mars.nasa.gov/news/'\n",
    "    browser.visit(url)\n",
    "    ##html = requests.get(url).text\n",
    "    time.sleep(5)\n",
    " #get the first new title and first news summary\n",
    "    html = browser.html\n",
    "\n",
    "\n",
    "    #get the htmle code form webpage and parse it\n",
    "    soup = BeautifulSoup(html,'html.parser')\n",
    "    try:\n",
    "\n",
    "        #get the first new title and first news summary\n",
    "        result = soup.find('div', class_ ='list_text')\n",
    "        news_title = result.find('div', target = 'content_title').text\n",
    "        news_link = result.a['href']\n",
    "        news_p = result.find('div', class_ ='article_teaser_body').text \n",
    "        news_date = result.find('div', class_ ='list_date').text \n",
    "        return news_link,news_p,news_date,news_title\n",
    "\n",
    "    ##except AttributeError:\n",
    "        ##return None,None,None,None\n",
    "    finally:\n",
    "        close_browser(browser)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import re\n",
    "from tweepy import OAuthHandler\n",
    "\n",
    "import twitter_credentials\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(twitter_credentials.Cons_API_key, twitter_credentials.Cons_API_secret_key )\n",
    "auth.set_access_token(twitter_credentials.Access_token, twitter_credentials.Access_token_secret)\n",
    "\n",
    "api = tweepy.API(auth)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    " api = tweepy.API(auth, parser=tweepy.parsers.JSONParser())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_user = \"MarsWxReport\"\n",
    "tweet = api.user_timeline(target_user, count =0, page = 0)\n",
    "link = ((tweet)[0]['entities']['urls'][0]['url'])\n",
    "mars_weather = ((tweet)[0]['text'])\n",
    "mars_weather = mars_weather.replace(link,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InSight sol 423 (2020-02-03) low -92.2ºC (-133.9ºF) high -9.9ºC (14.2ºF)\\nwinds from the S at 4.6 m/s (10.2 mph) gus… '"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mars_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = mars_weather,link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_browser():\n",
    "    #starting the browser\n",
    "    executable_path = {'executable_path':'/usr/local/bin/chromedriver'}\n",
    "    return Browser('chrome', **executable_path, headless=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link = ((tweet)[0]['entities']['urls'][0]['url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((tweet)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
